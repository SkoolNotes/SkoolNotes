#+TITLE: Story of Science: Computational Complexity Theory
#+CONTEXT: 21ISOS201
#+AUTHOR: Albert H
* sources
** gentle introductions
*** https://en.wikipedia.org/wiki/Computational_complexity_theory
*** https://complexityzoo.net/Petting_Zoo
* overview
** computational complexity theory studies how "difficult" a problem is
*** importantly, not how "good" an algorithm is... this field deals with all algorithms that solve a given problem
** key concepts
*** types of problems
*** Turing machines
*** reducibility
*** complexity classes
*** hierarchy
** key problems
*** P vs NP
* flows
** Wikipedia computational complexity theory
*** computational problems
**** problem instances
	 A problem describes the problem. the actual "numbers" that describe a specific problem is called a problem instance. sorting a list is a problem, sorting /this/ list is a problem instance.
**** representing problem instances
	 formally strings of characters from alphabets. The input size is the length of the string. Different representations can be chosen but it should be trivial (fast) to convert from one to the other.
**** decision problems (most basic type)
	 Generally, given an input, the output is either yes (accept) or no (reject). For example, deciding whether a graph is connected or not.
***** it can be thought of as a "formal language"                  :toexpand:
**** function problems
	 Very general: a function problem 'is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem'. Basically calculate a non-binary function.

	 Examples: traveling salesman, integer factorization.

	 However, all function problems can be modeled as decision problems: For some function $f(*args) \to ans$, it can be modeled as the decision problem of whether $(*args, ans)$ is a valid output.
***** but does this really work? how can a decision TM be used to compute the function output efficiently? :toexpand:
**** size of an instance
	 Size is usually the length of the input. The complexity is a function of the input size, usually representing the worst case time or space (or any other complexity measure) required for any input size.
*** machine models and complexity measures
**** Turing machine
	 standard Turing machine stuff. its very general. Many types of turing machines (probabilistic, non-deterministic, quantum, etc) are used to define different complexity classes.
**** other machine models                                          :toexpand:
	 Other non-standard Turing machines are used, but the idea is that they aren't actually any better, somehow?
**** Complexity Measures
	 Usually time or space, but any complexity measure that satisfies Blum's complexity axioms can be used. Examples include: communication complexity, circuit complexity.

	 Also constant factors don't really matter. And its usually the worst case.

	 Importantly, complexity measures are also a function of the type of Turing machine used, since some Turing machines are better in some scenarios.
***** blums complexity axioms                                      :toexpand:
**** best/worst/average case
	 We generally talk about worst case complexity, but some algorithms have good average-case which is good enough (eg. quicksort). Generally, best-case < average-case < amortized analysis < worst-case.
**** upper and lower bounds for problems
	 Importantly, this is *not an upper or lower bound for an algorithm*. Instead, for problems in general, it's relatively easy to decide an upper bound (which is just the worst case complexity of any correct algorithm), but a lower bound is difficult (since it must involve algorithms that haven't been discovered yet).
*** complexity classes
**** dependencies
	 Complexity classes are a function of the following factors
***** problem type
	  { decision, function, counting, optimization, promise, etc }
***** computation model
	  { deterministic Turing machine, non-deterministic, Boolean circuits, quantum TM, monotone circuits, etc }
***** bounded resources
	  { polynomial time, logarithmic space, constant depth }
**** an example definition
	 #+begin_quote
	 The set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)
	 #+end_quote

	 However, using a concrete function $f(n)$ is often computational-model-dependent, but the Cobham-Edmonds thesis states that 'the time complexities in any two reasonable general models of computation are polynomial related.'

	 This suggests that all if we want to be machine-independent, all polynomial problems are roughly the same and belong in the same class: P (for decision problems) and FP (for function problems).
***** why are there different classes if decision and function problems are the same-ish? dunno :toexpand:
**** important complexity classes
	 A nice list here but the complexity petting zoo is more friendly.
**** Hierarchy theorems                                            :toexpand:
	 We would like to establish a strict containment hierarchy within classes (but between different eg. polynomial functions). This does that, apparently?
**** Reduction
	 Many problems can be turned into other problems in their class, which provides an upper bound on the difficulty of the problem.

	 There are many types of reductions, but the most common type is the polynomial-time reduction which means the reduction takes polynomial time. If you take a non-polynomial reduction to turn a problem into a polynomial problem, then you haven't proven anything.
***** hardness and completeness
	  A problem $X$ is hard for a class $C$ if every problem in $C$ can be reduced to $X$. A problem $X$ is complete for $C$ if it is hard for $C$ it is in $C$. NP-complete problems are the "most difficult problems in NP" because other problems can be reduced to them.

	  Being able to reduce a hard problem to another problem shows that that other problem is just as hard, by contradiction. Similarly, being able to reduce a hard problem to a known easy one collapses the hierarchy.
*** important open problems
**** P vs NP
	 If any NP-complete problem can be reduced (polynomially) to a P problem, then many NP problems would be solvable in polynomial time. There are many NP problems that we would like to solve efficiently, so this would be a big deal.

	 In fact, many of the other 'important open problems' are important because they would show that $P \neq  NP$.
**** NP-indeterminate problems (in NP but not in P nor NP-complete) :toexpand:
	 some theorem shows that if P neq NP then there are NP-indeterminate problems. If we show that there are none, then that proves P = NP. Some unclassified problems (graph isomorphism problem, integer factorization problem) being NP-complete would 'collapse the polynomial hierarchy.' ?????
**** separations between other complexity classes
	 There are many classes that are improper subsets of each other. If any of those relations can be shown to be a proper subset, then classes on either side would be unequal. For example, many such relations exist between P and NP and showing that one of those relations is a proper subset relation would prove that P != NP. Or, proving that two classes (eg. P, PSPACE) are equal would squish all classes in between into one (in this case, showing that P = NP).
*** Intractability
	Meaning "not handleable". The Cobham-Edmonds thesis suggests that all polynomial problems are tractable.
