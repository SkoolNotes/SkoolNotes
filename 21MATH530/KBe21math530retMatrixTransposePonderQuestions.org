#+TITLE: Questions to Ponder about $A^TA$ and $AA^T$
* A real valued matrix

  Let $A =\begin{pmatrix}2 &1 \\ 3 & 2\end{pmatrix}$
  \[\begin{aligned}
  AA^T &= \begin{pmatrix}2 &1 \\ 3 & 2\end{pmatrix}\begin{pmatrix}2 &3 \\ 1 & 2\end{pmatrix} &=\begin{pmatrix}5 & 8 \\ 8 & 13\end{pmatrix}\\
  A^TA &= \begin{pmatrix}2 &3 \\ 1 & 2\end{pmatrix}\begin{pmatrix}2 &1 \\ 3 & 2\end{pmatrix} &=\begin{pmatrix}13 & 8 \\ 8 & 5\end{pmatrix}\\
  \end{aligned}\]

  \[\begin{aligned}
  \begin{pmatrix}a&b\\c&d\end{pmatrix}\begin{pmatrix}a&c\\b&d\end{pmatrix} =\begin{pmatrix}a^2+b^2 & ac+bd \\ ac+bd & c^2+d^2 \end{pmatrix}
  \end{aligned}\]
  Then, $A^TA$ is the same thing, but with $b, c$ swapped.
* For complex matrices

  \[\begin{aligned}
  \begin{pmatrix}a+bi & c+di \\ f+gi & j+ki\end{pmatrix} \begin{pmatrix}a+bi & f+gi \\ c+di & j+ki\end{pmatrix} =
  \begin{pmatrix}a^2-b^2+2abi + c^2-d^2+2cdi & af + agi + bfi - bg \\ af + agi + bfi - bg & f^2-g^2+2fgi + j^2-k^2+2jki\end{pmatrix}
  \end{aligned}\]

  I'm not sure if I'm noticing anything different from the real ones, although maybe the variables are just too confusing.
* Complex conjugate ($A^*A$ vs $A A^*$)

  \[\begin{aligned}
  \begin{pmatrix}a+bi & c+di \\ f+gi & j+ki \end{pmatrix}
  \begin{pmatrix}a-bi & f-gi \\ c-di & j-ki \end{pmatrix} =
  \begin{pmatrix} a^2 + b^2 + c^2 + d^2 & () \\ () & f^2 + g^2 + j^2 + k^2 \end{pmatrix}
  \end{aligned}\]

  \[\begin{aligned}
  \begin{pmatrix}a-bi & f-gi \\ c-di & j-ki \end{pmatrix}
  \begin{pmatrix}a+bi & c+di \\ f+gi & j+ki \end{pmatrix} =
  \begin{pmatrix} a^2 + b^2 + f^2 + g^2 & () \\ () & c^2 + d^2 + j^2 + k^2 \end{pmatrix}
  \end{aligned}\]

  The diagonals are real-valued, and the matrices are symmetric about the diagonal. I wonder if this means the matrices have identical eigenvalues... how do the diagonals of complex matricies change when they are upper-triangularized?
* Transpose distributivity with matrix multiplication

  \[\begin{aligned}
  (AB)^\top =\left(\begin{pmatrix}a&b\\c&d\end{pmatrix}\begin{pmatrix}w&x\\y&z\end{pmatrix}\right) ^\top
  =\begin{pmatrix}aw+by & cw+dy \\ ax + bz & cx + dz \end{pmatrix} =\begin{pmatrix}w&y\\x&z\end{pmatrix}\begin{pmatrix}a&c\\b&d\end{pmatrix} = B^\top A^\top
  \end{aligned}\]

  I have no good proof of this for larger matrices or non-square matrices, but it makes sense because both scalar addition and scalar multiplication are commutative and transposing swaps rows for columns. Thus, when a matrix on the left is multiplied by a matrix on the right, it is the same as the left matrix becoming the right matrix but after a transpose, because both operations swap the rows and columns in some sense so they "cancel out".
* Determinant distributivity with matrix multiplication

  \[\begin{aligned}
  \left|\begin{pmatrix}a&b\\c&d\end{pmatrix}\right|\left|\begin{pmatrix}w&x\\y&z\end{pmatrix}\right| = (ad-bc)(wz-xy)\\
  = adwz - adxy - bcwz + bcxy

   (aw+by)(cx+dz) - (ax+bz)(cw+dy) = \left| \begin{pmatrix}aw+by &  ax + bz\\ cw + dy & cx + dz\end{pmatrix} \right|
  \end{aligned}\]
